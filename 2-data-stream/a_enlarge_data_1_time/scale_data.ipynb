{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use faker (or something else) \n",
    "\n",
    "# increase the size of test dataset based on some rules or random settings \n",
    "# take the test data (with all its columns), and take the averages etc to generate further data beyond what is available\n",
    "\n",
    "# prepare data for streaming or store in google drive file system\n",
    "\n",
    "# results in TstTrId_fake.csv \n",
    "# stored in google drive api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
      "0        3663549       18403224           31.95         W  10409  111.0   \n",
      "1        3663550       18403263           49.00         W   4272  111.0   \n",
      "2        3663551       18403310          171.00         W   4476  574.0   \n",
      "3        3663552       18403310          284.95         W  10989  360.0   \n",
      "4        3663553       18403317           67.95         W  18018  452.0   \n",
      "\n",
      "   card3       card4  card5  card6  ...  id_31  id_32  id_33  id_34 id_35  \\\n",
      "0  150.0        visa  226.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
      "1  150.0        visa  226.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
      "2  150.0        visa  226.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
      "3  150.0        visa  166.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
      "4  150.0  mastercard  117.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
      "\n",
      "  id_36  id_37  id_38  DeviceType  DeviceInfo  \n",
      "0   NaN    NaN    NaN         NaN         NaN  \n",
      "1   NaN    NaN    NaN         NaN         NaN  \n",
      "2   NaN    NaN    NaN         NaN         NaN  \n",
      "3   NaN    NaN    NaN         NaN         NaN  \n",
      "4   NaN    NaN    NaN         NaN         NaN  \n",
      "\n",
      "[5 rows x 433 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# open pickle file\n",
    "\n",
    "# Load the pickle file\n",
    "file_path = './../processed_datasets/TstIdTr.pkl'\n",
    "TstIdTr = pd.read_pickle(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TransactionID',\n",
       " 'TransactionDT',\n",
       " 'TransactionAmt',\n",
       " 'ProductCD',\n",
       " 'card1',\n",
       " 'card2',\n",
       " 'card3',\n",
       " 'card4',\n",
       " 'card5',\n",
       " 'card6',\n",
       " 'addr1',\n",
       " 'addr2',\n",
       " 'dist1',\n",
       " 'dist2',\n",
       " 'P_emaildomain',\n",
       " 'R_emaildomain',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D13',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'M1',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'V1',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V5',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9',\n",
       " 'V10',\n",
       " 'V11',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V14',\n",
       " 'V15',\n",
       " 'V16',\n",
       " 'V17',\n",
       " 'V18',\n",
       " 'V19',\n",
       " 'V20',\n",
       " 'V21',\n",
       " 'V22',\n",
       " 'V23',\n",
       " 'V24',\n",
       " 'V25',\n",
       " 'V26',\n",
       " 'V27',\n",
       " 'V28',\n",
       " 'V29',\n",
       " 'V30',\n",
       " 'V31',\n",
       " 'V32',\n",
       " 'V33',\n",
       " 'V34',\n",
       " 'V35',\n",
       " 'V36',\n",
       " 'V37',\n",
       " 'V38',\n",
       " 'V39',\n",
       " 'V40',\n",
       " 'V41',\n",
       " 'V42',\n",
       " 'V43',\n",
       " 'V44',\n",
       " 'V45',\n",
       " 'V46',\n",
       " 'V47',\n",
       " 'V48',\n",
       " 'V49',\n",
       " 'V50',\n",
       " 'V51',\n",
       " 'V52',\n",
       " 'V53',\n",
       " 'V54',\n",
       " 'V55',\n",
       " 'V56',\n",
       " 'V57',\n",
       " 'V58',\n",
       " 'V59',\n",
       " 'V60',\n",
       " 'V61',\n",
       " 'V62',\n",
       " 'V63',\n",
       " 'V64',\n",
       " 'V65',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V68',\n",
       " 'V69',\n",
       " 'V70',\n",
       " 'V71',\n",
       " 'V72',\n",
       " 'V73',\n",
       " 'V74',\n",
       " 'V75',\n",
       " 'V76',\n",
       " 'V77',\n",
       " 'V78',\n",
       " 'V79',\n",
       " 'V80',\n",
       " 'V81',\n",
       " 'V82',\n",
       " 'V83',\n",
       " 'V84',\n",
       " 'V85',\n",
       " 'V86',\n",
       " 'V87',\n",
       " 'V88',\n",
       " 'V89',\n",
       " 'V90',\n",
       " 'V91',\n",
       " 'V92',\n",
       " 'V93',\n",
       " 'V94',\n",
       " 'V95',\n",
       " 'V96',\n",
       " 'V97',\n",
       " 'V98',\n",
       " 'V99',\n",
       " 'V100',\n",
       " 'V101',\n",
       " 'V102',\n",
       " 'V103',\n",
       " 'V104',\n",
       " 'V105',\n",
       " 'V106',\n",
       " 'V107',\n",
       " 'V108',\n",
       " 'V109',\n",
       " 'V110',\n",
       " 'V111',\n",
       " 'V112',\n",
       " 'V113',\n",
       " 'V114',\n",
       " 'V115',\n",
       " 'V116',\n",
       " 'V117',\n",
       " 'V118',\n",
       " 'V119',\n",
       " 'V120',\n",
       " 'V121',\n",
       " 'V122',\n",
       " 'V123',\n",
       " 'V124',\n",
       " 'V125',\n",
       " 'V126',\n",
       " 'V127',\n",
       " 'V128',\n",
       " 'V129',\n",
       " 'V130',\n",
       " 'V131',\n",
       " 'V132',\n",
       " 'V133',\n",
       " 'V134',\n",
       " 'V135',\n",
       " 'V136',\n",
       " 'V137',\n",
       " 'V138',\n",
       " 'V139',\n",
       " 'V140',\n",
       " 'V141',\n",
       " 'V142',\n",
       " 'V143',\n",
       " 'V144',\n",
       " 'V145',\n",
       " 'V146',\n",
       " 'V147',\n",
       " 'V148',\n",
       " 'V149',\n",
       " 'V150',\n",
       " 'V151',\n",
       " 'V152',\n",
       " 'V153',\n",
       " 'V154',\n",
       " 'V155',\n",
       " 'V156',\n",
       " 'V157',\n",
       " 'V158',\n",
       " 'V159',\n",
       " 'V160',\n",
       " 'V161',\n",
       " 'V162',\n",
       " 'V163',\n",
       " 'V164',\n",
       " 'V165',\n",
       " 'V166',\n",
       " 'V167',\n",
       " 'V168',\n",
       " 'V169',\n",
       " 'V170',\n",
       " 'V171',\n",
       " 'V172',\n",
       " 'V173',\n",
       " 'V174',\n",
       " 'V175',\n",
       " 'V176',\n",
       " 'V177',\n",
       " 'V178',\n",
       " 'V179',\n",
       " 'V180',\n",
       " 'V181',\n",
       " 'V182',\n",
       " 'V183',\n",
       " 'V184',\n",
       " 'V185',\n",
       " 'V186',\n",
       " 'V187',\n",
       " 'V188',\n",
       " 'V189',\n",
       " 'V190',\n",
       " 'V191',\n",
       " 'V192',\n",
       " 'V193',\n",
       " 'V194',\n",
       " 'V195',\n",
       " 'V196',\n",
       " 'V197',\n",
       " 'V198',\n",
       " 'V199',\n",
       " 'V200',\n",
       " 'V201',\n",
       " 'V202',\n",
       " 'V203',\n",
       " 'V204',\n",
       " 'V205',\n",
       " 'V206',\n",
       " 'V207',\n",
       " 'V208',\n",
       " 'V209',\n",
       " 'V210',\n",
       " 'V211',\n",
       " 'V212',\n",
       " 'V213',\n",
       " 'V214',\n",
       " 'V215',\n",
       " 'V216',\n",
       " 'V217',\n",
       " 'V218',\n",
       " 'V219',\n",
       " 'V220',\n",
       " 'V221',\n",
       " 'V222',\n",
       " 'V223',\n",
       " 'V224',\n",
       " 'V225',\n",
       " 'V226',\n",
       " 'V227',\n",
       " 'V228',\n",
       " 'V229',\n",
       " 'V230',\n",
       " 'V231',\n",
       " 'V232',\n",
       " 'V233',\n",
       " 'V234',\n",
       " 'V235',\n",
       " 'V236',\n",
       " 'V237',\n",
       " 'V238',\n",
       " 'V239',\n",
       " 'V240',\n",
       " 'V241',\n",
       " 'V242',\n",
       " 'V243',\n",
       " 'V244',\n",
       " 'V245',\n",
       " 'V246',\n",
       " 'V247',\n",
       " 'V248',\n",
       " 'V249',\n",
       " 'V250',\n",
       " 'V251',\n",
       " 'V252',\n",
       " 'V253',\n",
       " 'V254',\n",
       " 'V255',\n",
       " 'V256',\n",
       " 'V257',\n",
       " 'V258',\n",
       " 'V259',\n",
       " 'V260',\n",
       " 'V261',\n",
       " 'V262',\n",
       " 'V263',\n",
       " 'V264',\n",
       " 'V265',\n",
       " 'V266',\n",
       " 'V267',\n",
       " 'V268',\n",
       " 'V269',\n",
       " 'V270',\n",
       " 'V271',\n",
       " 'V272',\n",
       " 'V273',\n",
       " 'V274',\n",
       " 'V275',\n",
       " 'V276',\n",
       " 'V277',\n",
       " 'V278',\n",
       " 'V279',\n",
       " 'V280',\n",
       " 'V281',\n",
       " 'V282',\n",
       " 'V283',\n",
       " 'V284',\n",
       " 'V285',\n",
       " 'V286',\n",
       " 'V287',\n",
       " 'V288',\n",
       " 'V289',\n",
       " 'V290',\n",
       " 'V291',\n",
       " 'V292',\n",
       " 'V293',\n",
       " 'V294',\n",
       " 'V295',\n",
       " 'V296',\n",
       " 'V297',\n",
       " 'V298',\n",
       " 'V299',\n",
       " 'V300',\n",
       " 'V301',\n",
       " 'V302',\n",
       " 'V303',\n",
       " 'V304',\n",
       " 'V305',\n",
       " 'V306',\n",
       " 'V307',\n",
       " 'V308',\n",
       " 'V309',\n",
       " 'V310',\n",
       " 'V311',\n",
       " 'V312',\n",
       " 'V313',\n",
       " 'V314',\n",
       " 'V315',\n",
       " 'V316',\n",
       " 'V317',\n",
       " 'V318',\n",
       " 'V319',\n",
       " 'V320',\n",
       " 'V321',\n",
       " 'V322',\n",
       " 'V323',\n",
       " 'V324',\n",
       " 'V325',\n",
       " 'V326',\n",
       " 'V327',\n",
       " 'V328',\n",
       " 'V329',\n",
       " 'V330',\n",
       " 'V331',\n",
       " 'V332',\n",
       " 'V333',\n",
       " 'V334',\n",
       " 'V335',\n",
       " 'V336',\n",
       " 'V337',\n",
       " 'V338',\n",
       " 'V339',\n",
       " 'id_01',\n",
       " 'id_02',\n",
       " 'id_03',\n",
       " 'id_04',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'id_07',\n",
       " 'id_08',\n",
       " 'id_09',\n",
       " 'id_10',\n",
       " 'id_11',\n",
       " 'id_12',\n",
       " 'id_13',\n",
       " 'id_14',\n",
       " 'id_15',\n",
       " 'id_16',\n",
       " 'id_17',\n",
       " 'id_18',\n",
       " 'id_19',\n",
       " 'id_20',\n",
       " 'id_21',\n",
       " 'id_22',\n",
       " 'id_23',\n",
       " 'id_24',\n",
       " 'id_25',\n",
       " 'id_26',\n",
       " 'id_27',\n",
       " 'id_28',\n",
       " 'id_29',\n",
       " 'id_30',\n",
       " 'id_31',\n",
       " 'id_32',\n",
       " 'id_33',\n",
       " 'id_34',\n",
       " 'id_35',\n",
       " 'id_36',\n",
       " 'id_37',\n",
       " 'id_38',\n",
       " 'DeviceType',\n",
       " 'DeviceInfo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(TstIdTr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now, I want you to use faker or some other type of method to simulate, recreate, syntehsise this data (in dataframe TstIdTr, test data from fraud detectiondataset). \n",
    "# # i want you to populate a new dataframe with the identical columns \n",
    "# # the new dataframe is called TstIdTr_synth\n",
    "# # the new dataframe should have about 50000 rows \n",
    "# # i need tyou to analyse each column (i hvae shown you here which are categoriacl, which numerical) and recreate the type of data there\n",
    "# # i don't watn the same data. i want similar data. \n",
    "# # i need you to recreate the sparseness of the data, so if there are nans, then that should be allowed in the faked data\n",
    "# # for categorical, i wnat categories that exist already, not made up ones. there are enough examples in TstIdTr to fill this in)\n",
    "\n",
    "# # test dataframe existing: TstIdTr\n",
    "\n",
    "# # to identify featrures by type:\n",
    "# # Categorical features\n",
    "# categorical_features = [\n",
    "#     'ProductCD',\n",
    "#     'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "#     'addr1', 'addr2',\n",
    "#     'P_emaildomain', 'R_emaildomain',\n",
    "#     'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "#     'DeviceType', 'DeviceInfo',\n",
    "#     'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', \n",
    "#     'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', \n",
    "#     'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', \n",
    "#     'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'\n",
    "# ]\n",
    "\n",
    "# # Numerical features (excluding TransactionID and isFraud)\n",
    "# numerical_features = [\n",
    "#     col for col in trIdTr.columns if col not in categorical_features + ['TransactionID']\n",
    "# ]\n",
    "\n",
    "# # , 'isFraud' would uusally be in there, but since this is test, we don't have it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Set up Faker instance\n",
    "fake = Faker()\n",
    "\n",
    "# Load your existing dataset (TstIdTr)\n",
    "# Example path, adjust based on your setup:\n",
    "# TstIdTr = pd.read_pickle('./../processed_datasets/TstIdTr.pkl')\n",
    "\n",
    "# Define number of rows for the synthetic data\n",
    "num_synth_rows = 150000\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = [\n",
    "    'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', \n",
    "    'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'DeviceInfo',\n",
    "    'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', \n",
    "    'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', \n",
    "    'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', \n",
    "    'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'\n",
    "]\n",
    "\n",
    "# Numerical features\n",
    "numerical_features = [\n",
    "    col for col in TstIdTr.columns \n",
    "    if col not in categorical_features + ['TransactionID']\n",
    "]\n",
    "\n",
    "# Initialize the synthetic DataFrame\n",
    "TstIdTr_synth = pd.DataFrame()\n",
    "\n",
    "# Populate categorical features\n",
    "for cat_feat in categorical_features:\n",
    "    unique_values = TstIdTr[cat_feat].dropna().unique()  # Unique non-NaN values from the original\n",
    "    nan_probability = TstIdTr[cat_feat].isna().mean()    # Probability of NaNs\n",
    "    TstIdTr_synth[cat_feat] = [\n",
    "        random.choice(unique_values) if random.random() > nan_probability else np.nan\n",
    "        for _ in range(num_synth_rows)\n",
    "    ]\n",
    "\n",
    "# Populate numerical features\n",
    "for num_feat in numerical_features:\n",
    "    mean = TstIdTr[num_feat].mean()\n",
    "    std = TstIdTr[num_feat].std()\n",
    "    nan_probability = TstIdTr[num_feat].isna().mean()  # Probability of NaNs\n",
    "    \n",
    "    # Generate synthetic numerical data\n",
    "    TstIdTr_synth[num_feat] = [\n",
    "        random.gauss(mean, std) if random.random() > nan_probability else np.nan\n",
    "        for _ in range(num_synth_rows)\n",
    "    ]\n",
    "\n",
    "# Add a synthetic TransactionID column\n",
    "# Find the starting point for TransactionID\n",
    "max_transaction_id = TstIdTr['TransactionID'].max()  # Get the max value in the test set\n",
    "\n",
    "# Add a synthetic TransactionID column\n",
    "TstIdTr_synth['TransactionID'] = range(max_transaction_id + 1000, max_transaction_id + 1 + num_synth_rows)\n",
    "\n",
    "# Validate the structure\n",
    "print(TstIdTr_synth.info())\n",
    "\n",
    "# Save to file if needed\n",
    "TstIdTr_synth.to_pickle('./../processed_datasets/TstIdTr_synth.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fake or other library to create daataset with same columns as this file above\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
