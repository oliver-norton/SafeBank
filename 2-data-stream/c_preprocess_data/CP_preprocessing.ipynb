{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT STREAMED DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stream the data, bit by bit (it is precombined, a combo of faker data + test data)\n",
    "\n",
    "# collect from a kaftka producer \n",
    "\n",
    "# google api \n",
    "\n",
    "# watoomi data - 1000 rows of each per 30 seconds \n",
    "# TstIdTr.pkl\n",
    "# TstIdTr_synth.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "categorical_features = [\n",
    "    'ProductCD',\n",
    "    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2',\n",
    "    'P_emaildomain', 'R_emaildomain',\n",
    "    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "    'DeviceType', 'DeviceInfo',\n",
    "    'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', \n",
    "    'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', \n",
    "    'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', \n",
    "    'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'\n",
    "]\n",
    "\n",
    "# Numerical features (excluding TransactionID and isFraud)\n",
    "numerical_features = [\n",
    "    col for col in trIdTr.columns if col not in categorical_features + ['TransactionID', 'isFraud']\n",
    "\n",
    "       # categorical\n",
    "\n",
    "## label encoding for all categorical features \n",
    "## set all categories as categorical pandas format\n",
    "\n",
    "# # Label Encoding for all categorical features\n",
    "# for col in categorical_features:\n",
    "#     if trIdTr[col].dtype == 'object' or col in categorical_features:\n",
    "#         le = LabelEncoder()\n",
    "#         trIdTr[col] = le.fit_transform(trIdTr[col].astype(str))\n",
    "\n",
    "\n",
    "# Assuming label encoders for categorical features were saved during training\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # Fit encoder on training data and apply to test data\n",
    "    le.fit(trIdTr[col])  # Assuming `trIdTr[col]` is from the training set\n",
    "    test_data[col] = le.transform(test_data[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't perform target encoding during testing; use the target-encoded features stored during training.\n",
    "\n",
    "# Assuming target encoding was done on training set, apply the same encoding to test data\n",
    "for col in high_cardinality_features:\n",
    "    test_data[f'{col}_target_enc'] = test_data[col].map(target_encoding_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# Setting all as categorical pandas format\n",
    "for col in categorical_features:\n",
    "    trIdTr[col] = trIdTr[col].astype('category')\n",
    "]\n",
    "\n",
    "## identify cardinality of categorical features \n",
    "## do target encoding for high cardinality categoricals\n",
    "## add into dataframe \n",
    "\n",
    "# Calculate cardinality for categorical features\n",
    "cardinality = {col: trIdTr[col].nunique() for col in categorical_features}\n",
    "print(\"Cardinality of categorical features:\", cardinality)\n",
    "\n",
    "# Define a threshold for high cardinality\n",
    "\n",
    "## HERE WE SHOULD MANUALLY INCLUDE CHANGES\n",
    "high_cardinality_threshold = 20  # Features with more than 20 unique categories are considered high cardinality\n",
    "low_cardinality_features = [col for col, unique_vals in cardinality.items() if unique_vals <= high_cardinality_threshold]\n",
    "high_cardinality_features = [col for col, unique_vals in cardinality.items() if unique_vals > high_cardinality_threshold]\n",
    "\n",
    "# Set low cardinality features as 'category' dtype (LightGBM will handle them natively)\n",
    "for col in low_cardinality_features:\n",
    "    trIdTr[col] = trIdTr[col].astype('category')\n",
    "\n",
    "# Perform target encoding for high-cardinality features\n",
    "# Ensure 'isFraud' is present\n",
    "assert 'isFraud' in trIdTr.columns, \"The 'isFraud' target column is missing.\"\n",
    "\n",
    "for col in high_cardinality_features:\n",
    "    # Initialize target encoded column\n",
    "    trIdTr[f'{col}_target_enc'] = np.nan\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(trIdTr, trIdTr['isFraud']):\n",
    "        train_data, val_data = trIdTr.iloc[train_idx], trIdTr.iloc[val_idx]\n",
    "        \n",
    "        # Compute target encoding (mean of 'isFraud' for each category)\n",
    "        means = train_data.groupby(col)['isFraud'].mean()\n",
    "        \n",
    "        # Map the target encoding to the validation set\n",
    "        trIdTr.loc[val_idx, f'{col}_target_enc'] = val_data[col].map(means)\n",
    "    \n",
    "    # For any remaining NaN values, we leave them as NaN (LightGBM can handle them)\n",
    "    # No need to fill NaNs since LightGBM handles them naturally\n",
    "\n",
    "print(\"Target encoding added for high-cardinality features:\", high_cardinality_features)\n",
    "\n",
    "# trIdTr.to_csv('./processed_datasets/trIdTr1.csv')\n",
    "# trIdTr = pd.read_csv('./processed_datasets/trIdTr1.csv')\n",
    "\n",
    "# feature engineering \n",
    "\n",
    "### add in TransactionDT enriched columns:\n",
    "\n",
    "### numerical \n",
    "trIdTr['TransactionDT_days'] = trIdTr['TransactionDT'] // (24 * 60 * 60)  # Convert to days \n",
    "\n",
    "### categorical\n",
    "\n",
    "trIdTr['TransactionDT_weekday'] = trIdTr['TransactionDT_days'] % 7  # Day of the week (0-6)\n",
    "trIdTr['TransactionDT_hour'] = (trIdTr['TransactionDT']//3600) % 24  # Calculate hour of the day (0-23)\n",
    "\n",
    "### numerical\n",
    "trIdTr['TransactionDT_hours'] = (trIdTr['TransactionDT'] // (60 * 60)) % 24  # Extract hours - hours since reference point in dataset\n",
    "\n",
    "trIdTr['hour_sin'] = np.sin(2 * np.pi * trIdTr['TransactionDT_hour'] / 24)\n",
    "trIdTr['hour_cos'] = np.cos(2 * np.pi * trIdTr['TransactionDT_hour'] / 24)\n",
    "trIdTr['weekday_sin'] = np.sin(2 * np.pi * trIdTr['TransactionDT_weekday'] / 7)\n",
    "trIdTr['weekday_cos'] = np.cos(2 * np.pi * trIdTr['TransactionDT_weekday'] / 7)\n",
    "\n",
    "####    TransactionAmt split into whole dollars and cents (so one dollars/whole number, and one cents column)\n",
    "trIdTr['TransactionAmt_dollars'] = trIdTr['TransactionAmt'] // 1  # Extract whole dollars\n",
    "trIdTr['TransactionAmt_cents'] = (trIdTr['TransactionAmt'] * 100) % 100  # Extract cents\n",
    "\n",
    "# update the feature sets: \n",
    "\n",
    "### categorical_features\n",
    "### numerical_features , \n",
    "\n",
    "# Update categorical_features with newly created categorical columns\n",
    "categorical_features += [\n",
    "    'TransactionDT_weekday', 'TransactionDT_hour',  # New categorical features\n",
    "]\n",
    "\n",
    "# Update numerical_features with newly created numerical columns\n",
    "numerical_features += [\n",
    "    'TransactionDT_days', 'TransactionDT_hours',  # Existing numerical features\n",
    "    'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',  # Cyclical features\n",
    "    'TransactionAmt_dollars', 'TransactionAmt_cents',  # New features from TransactionAmt\n",
    "    'DeviceInfo_target_enc', 'id_30_target_enc', 'id_25_target_enc', 'id_31_target_enc', \n",
    "    'id_20_target_enc', 'addr2_target_enc', 'card5_target_enc', 'card2_target_enc', \n",
    "    'addr1_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'id_21_target_enc', \n",
    "    'card3_target_enc', 'id_17_target_enc', 'P_emaildomain_target_enc', 'id_33_target_enc', \n",
    "    'id_22_target_enc', 'id_26_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc',\n",
    "    'DeviceInfo_target_enc', 'id_30_target_enc', 'id_25_target_enc', 'id_31_target_enc', \n",
    "    'id_20_target_enc', 'addr2_target_enc', 'card5_target_enc', 'card2_target_enc', \n",
    "    'addr1_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'id_21_target_enc', \n",
    "    'card3_target_enc', 'id_17_target_enc', 'P_emaildomain_target_enc', 'id_33_target_enc', \n",
    "    'id_22_target_enc', 'id_26_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc', \n",
    "    'id_14_target_enc'\n",
    "]\n",
    "\n",
    "\n",
    "# Check the final lists\n",
    "print(\"Updated categorical features:\", categorical_features)\n",
    "print(\"Updated numerical features:\", numerical_features)\n",
    "\n",
    "### note TransactionDT_hours,TransactionDT_days, are not important features so should not be put into either numieracl or categorerial (but still remain in dataset)\n",
    "\n",
    "### find set which are not in either\n",
    "\n",
    "# Get all columns from the dataframe\n",
    "all_columns = set(trIdTr.columns)\n",
    "\n",
    "# Combine categorical and numerical features\n",
    "all_features = set(categorical_features + numerical_features)\n",
    "\n",
    "# Identify features that are not in either categorical or numerical lists\n",
    "missing_features = all_columns - all_features\n",
    "\n",
    "# Print the missing features\n",
    "print(\"Features not included in either categorical or numerical features:\", missing_features)\n",
    "\n",
    "# remove unnecessary columns \n",
    "\n",
    "# remove unnecessary columns - go through \n",
    "### categorical_features: any features with target_enc in numerical_features set\n",
    "### numerical_features: remove TransactionDT_hours, TransactionDT_days\n",
    "\n",
    "# check that all columns are in categorical_features or numerical_features; if not, remove column, print removed column names. \n",
    "# exclusions are: 'Unnamed: 0', 'isFraud', 'TransactionID'\n",
    "\n",
    "# Remove target-encoded features from categorical_features if they exist in numerical_features\n",
    "categorical_features = [col for col in categorical_features if col + '_target_enc' not in numerical_features]\n",
    "\n",
    "# Remove TransactionDT_hours and TransactionDT_days from numerical_features\n",
    "numerical_features = [col for col in numerical_features if col not in ['TransactionDT_hours', 'TransactionDT_days']]\n",
    "\n",
    "\n",
    "# Create a set of all columns that should be included\n",
    "all_columns = set(categorical_features + numerical_features)\n",
    "\n",
    "# Exclude specific columns from being considered\n",
    "excluded_columns = {'Unnamed: 0', 'isFraud', 'TransactionID'}\n",
    "\n",
    "# Remove any columns from the dataframe that are not in the updated feature sets\n",
    "removed_columns = []\n",
    "for col in trIdTr.columns:\n",
    "    if col not in all_columns and col not in excluded_columns:\n",
    "        removed_columns.append(col)\n",
    "        trIdTr.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Print the names of removed columns\n",
    "print(\"Removed columns:\", removed_columns)\n",
    "\n",
    "# Verify that all remaining columns are accounted for\n",
    "remaining_columns = set(trIdTr.columns)\n",
    "assert all(col in all_columns or col in excluded_columns for col in remaining_columns), \"Some columns are not in either feature list!\"\n",
    "\n",
    "# Remove 'TransactionDT' and 'TransactionAmt' from numerical_features\n",
    "numerical_features = [col for col in numerical_features if col not in ['TransactionDT', 'TransactionAmt']]\n",
    "\n",
    "# Drop 'TransactionDT' and 'TransactionAmt' from the DataFrame\n",
    "trIdTr.drop(['TransactionDT', 'TransactionAmt'], axis=1, inplace=True)\n",
    "\n",
    "# create copy\n",
    "trIdTr2 = trIdTr.copy()\n",
    "\n",
    "## combining features: correlation with target\n",
    "\n",
    "### evaluate which features are and aren't correlated with target isFraud\n",
    "\n",
    "\n",
    "# Check correlation for numerical features with 'isFraud' (using Pearson's correlation)\n",
    "numerical_corr = trIdTr[numerical_features].corrwith(trIdTr['isFraud']).sort_values(ascending=False)\n",
    "print(\"Numerical Features Correlation with isFraud:\")\n",
    "print(numerical_corr)\n",
    "\n",
    "# Plot a heatmap of numerical correlations with target for better visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(numerical_corr.to_frame(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation of Numerical Features with Target ('isFraud')\")\n",
    "plt.show()\n",
    "\n",
    "## remove uncorrelated features (with isFraud)\n",
    "\n",
    "# Set a threshold for weak correlation (e.g., below 0.1 or -0.1)\n",
    "low_correlation_features = numerical_corr[abs(numerical_corr) < 0.1].index\n",
    "\n",
    "# Remove features with low correlation\n",
    "numerical_features = [feature for feature in numerical_features if feature not in low_correlation_features]\n",
    "\n",
    "# Print features to be removed\n",
    "print(f\"Features with low correlation (removed): {low_correlation_features}\")\n",
    "\n",
    "## update trIdTr2 to include features only needed:\n",
    "\n",
    "# Define the columns to include\n",
    "included_columns = ['isFraud', 'Unnamed: 0', 'TransactionID'] + numerical_features + categorical_features\n",
    "\n",
    "# Update trIdTr2 by selecting only the desired columns\n",
    "trIdTr2 = trIdTr[included_columns]\n",
    "\n",
    "# Print the shape of the updated dataframe\n",
    "print(f\"Updated dataframe shape: {trIdTr2.shape}\")\n",
    "\n",
    "#create copy\n",
    "trIdTr3 = trIdTr2.copy()\n",
    "\n",
    "# Identify duplicated columns\n",
    "duplicate_columns = trIdTr3.columns[trIdTr3.columns.duplicated()]\n",
    "\n",
    "# Log the duplicated columns\n",
    "print(f\"Duplicated columns: {duplicate_columns.tolist()}\")\n",
    "\n",
    "# Drop duplicate columns\n",
    "trIdTr3 = trIdTr3.loc[:, ~trIdTr3.columns.duplicated()]\n",
    "\n",
    "# train the lightgbm model\n",
    "\n",
    "## set up parameter and train\n",
    "### special note: early_stopping and log_evaluation should be callbacks \n",
    "### we use stratifiedKfold to reduce effect of imbalanced classsee \n",
    "\n",
    "\n",
    "\n",
    "# Prepare the features and target\n",
    "X = trIdTr3.drop(columns=['isFraud', 'Unnamed: 0', 'TransactionID'])\n",
    "y = trIdTr3['isFraud']\n",
    "\n",
    "# Set up StratifiedKFold (5-fold)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',             # Binary classification\n",
    "    'metric': 'auc',                   # Evaluation metric (AUC)\n",
    "    'boosting_type': 'gbdt',           # Gradient Boosting Decision Tree\n",
    "    'num_leaves': 31,                  # Number of leaves in one tree\n",
    "    'learning_rate': 0.05,             # Learning rate\n",
    "    'feature_fraction': 0.9,           # Fraction of features to be used in each iteration\n",
    "    'bagging_fraction': 0.8,           # Fraction of data to be used in each iteration\n",
    "    'bagging_freq': 5,                 # Frequency of bagging\n",
    "    'verbose': -1                      # Suppress LightGBM output\n",
    "}\n",
    "\n",
    "# Early stopping and log evaluation callbacks\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=50),  # Stop if validation AUC does not improve for 50 rounds\n",
    "    lgb.log_evaluation(period=10)            # Log the evaluation every 10 rounds\n",
    "]\n",
    "\n",
    "# Initialize variables to store the out-of-fold predictions and actual labels\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "roc_auc_scores = []\n",
    "\n",
    "# Perform StratifiedKFold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Training fold {fold+1}...\")\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Convert to LightGBM dataset format\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'valid'],\n",
    "        num_boost_round=1000,                # Maximum number of boosting rounds\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Store the out-of-fold predictions\n",
    "    oof_preds[val_idx] = y_pred\n",
    "    \n",
    "    # Calculate and print ROC AUC score for the fold\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    print(f\"Fold {fold+1} ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Calculate and print the average ROC AUC across all folds\n",
    "avg_roc_auc = np.mean(roc_auc_scores)\n",
    "print(f\"\\nAverage ROC AUC across all folds: {avg_roc_auc:.4f}\")\n",
    "\n",
    "# Save the model to a file\n",
    "model.save_model('safeBank_lightGBM_model.txt')\n",
    "\n",
    "\n",
    "\n",
    "# predict here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
