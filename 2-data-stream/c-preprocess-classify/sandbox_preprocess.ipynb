{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages\n"
     ]
    }
   ],
   "source": [
    "print('Importing packages')\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import chi2_contingency\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_ = pd.read_csv('tsIdTr_ex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10958/2329938852.py:5: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  tsIdTr_synth = pkl.load(file)\n"
     ]
    }
   ],
   "source": [
    "# with open('/home/watoomi/SafeBank/1-model/for_g_drive/tsIdTr_base.pkl', 'rb') as file:\n",
    "#         tsIdTr_base = pkl.load(file)\n",
    "\n",
    "with open('/home/watoomi/SafeBank/1-model/for_g_drive/TsIdTr_synth.pkl', 'rb') as file:\n",
    "        tsIdTr_synth = pkl.load(file)\n",
    "\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tsIdTr_synth[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsIdTr = tsIdTr_synth[0:100].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ProductCD  card1  card2  card3             card4  card5        card6  \\\n",
      "0          W  17568  107.0  210.0  american express  192.0  charge card   \n",
      "1          C   3744  530.0  145.0          discover  169.0        debit   \n",
      "2          S   3082  462.0  114.0        mastercard  107.0       credit   \n",
      "3          R   2560  594.0  203.0              visa  199.0       credit   \n",
      "4          S   8347  456.0  119.0          discover  223.0       credit   \n",
      "..       ...    ...    ...    ...               ...    ...          ...   \n",
      "95         C  13458  588.0  223.0              visa  130.0        debit   \n",
      "96         S   6576  456.0  133.0        mastercard  199.0        debit   \n",
      "97         C  14705  270.0  174.0        mastercard  107.0  charge card   \n",
      "98         W   5113  127.0  231.0        mastercard  117.0        debit   \n",
      "99         S   4744  135.0  110.0          discover  190.0       credit   \n",
      "\n",
      "    addr1  addr2  P_emaildomain  ...    id_03     id_04     id_05      id_06  \\\n",
      "0   473.0   34.0     icloud.com  ...      NaN       NaN       NaN        NaN   \n",
      "1   406.0   98.0        aol.com  ...      NaN -0.825353       NaN        NaN   \n",
      "2   333.0   88.0    netzero.net  ...      NaN       NaN       NaN  15.306632   \n",
      "3   175.0   78.0            NaN  ...      NaN -0.404457       NaN        NaN   \n",
      "4   260.0   27.0      ymail.com  ...  0.23811  0.277727       NaN -15.533625   \n",
      "..    ...    ...            ...  ...      ...       ...       ...        ...   \n",
      "95  287.0   81.0  bellsouth.net  ...      NaN  0.746520       NaN        NaN   \n",
      "96  343.0    NaN        aim.com  ...      NaN       NaN       NaN        NaN   \n",
      "97  252.0   79.0    outlook.com  ...      NaN       NaN  0.461067  -3.686741   \n",
      "98  212.0   77.0         me.com  ...      NaN       NaN       NaN        NaN   \n",
      "99  208.0    NaN       mail.com  ...      NaN       NaN       NaN        NaN   \n",
      "\n",
      "   id_07 id_08     id_09     id_10       id_11 TransactionID  \n",
      "0    NaN   NaN -0.588454       NaN         NaN       4171239  \n",
      "1    NaN   NaN       NaN       NaN         NaN       4171240  \n",
      "2    NaN   NaN       NaN -2.479183  100.530960       4171241  \n",
      "3    NaN   NaN       NaN       NaN         NaN       4171242  \n",
      "4    NaN   NaN       NaN       NaN  100.586986       4171243  \n",
      "..   ...   ...       ...       ...         ...           ...  \n",
      "95   NaN   NaN       NaN       NaN         NaN       4171334  \n",
      "96   NaN   NaN       NaN       NaN   99.068887       4171335  \n",
      "97   NaN   NaN       NaN       NaN         NaN       4171336  \n",
      "98   NaN   NaN       NaN       NaN   99.381180       4171337  \n",
      "99   NaN   NaN       NaN       NaN         NaN       4171338  \n",
      "\n",
      "[100 rows x 433 columns]\n",
      "/home/watoomi/SafeBank/2-data-stream/c-preprocess-classify\n",
      "Updated columns in tsIdTr: ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'DeviceInfo', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'TransactionDT', 'TransactionAmt', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'TransactionID']\n",
      "Columns in tsIdTr but not in loaded_columns: set()\n",
      "Columns in loaded_columns but not in tsIdTr: {'isFraud'}\n",
      "All sets exclude isFraud, for testing\n",
      "Classes for ProductCD: ['C' 'H' 'R' 'S' 'W']\n",
      "Mapping for ProductCD: {'C': 0, 'H': 1, 'R': 2, 'S': 3, 'W': 4}\n",
      "ProductCD         object\n",
      "card1              int64\n",
      "card2            float64\n",
      "card3            float64\n",
      "card4             object\n",
      "                  ...   \n",
      "id_08            float64\n",
      "id_09            float64\n",
      "id_10            float64\n",
      "id_11            float64\n",
      "TransactionID      int64\n",
      "Length: 433, dtype: object\n",
      "type of dataframe is <class 'pandas.core.frame.DataFrame'>\n",
      "Applying label encoding to feature: ProductCD\n",
      "Applying label encoding to feature: card4\n",
      "Applying label encoding to feature: card6\n",
      "Applying label encoding to feature: M1\n",
      "Applying label encoding to feature: M2\n",
      "Applying label encoding to feature: M3\n",
      "Applying label encoding to feature: M4\n",
      "Applying label encoding to feature: M5\n",
      "Applying label encoding to feature: M6\n",
      "Applying label encoding to feature: M7\n",
      "Applying label encoding to feature: M8\n",
      "Applying label encoding to feature: M9\n",
      "Applying label encoding to feature: DeviceType\n",
      "Applying label encoding to feature: id_12\n",
      "Applying label encoding to feature: id_15\n",
      "Applying label encoding to feature: id_16\n",
      "Applying label encoding to feature: id_18\n",
      "Applying label encoding to feature: id_23\n",
      "Applying label encoding to feature: id_24\n",
      "Applying label encoding to feature: id_27\n",
      "Applying label encoding to feature: id_28\n",
      "Applying label encoding to feature: id_29\n",
      "Applying label encoding to feature: id_32\n",
      "Unseen labels in id_32: {'48.0', '8.0'}\n",
      "Applying label encoding to feature: id_34\n",
      "Applying label encoding to feature: id_35\n",
      "Applying label encoding to feature: id_36\n",
      "Applying label encoding to feature: id_37\n",
      "Applying label encoding to feature: id_38\n",
      "Classes for id_18: ['10.0' '11.0' '12.0' '13.0' '14.0' '15.0' '16.0' '17.0' '18.0' '20.0'\n",
      " '21.0' '23.0' '24.0' '25.0' '26.0' '27.0' '28.0' '29.0' 'nan']\n",
      "Mapping for id_18: {np.str_('10.0'): 0, np.str_('11.0'): 1, np.str_('12.0'): 2, np.str_('13.0'): 3, np.str_('14.0'): 4, np.str_('15.0'): 5, np.str_('16.0'): 6, np.str_('17.0'): 7, np.str_('18.0'): 8, np.str_('20.0'): 9, np.str_('21.0'): 10, np.str_('23.0'): 11, np.str_('24.0'): 12, np.str_('25.0'): 13, np.str_('26.0'): 14, np.str_('27.0'): 15, np.str_('28.0'): 16, np.str_('29.0'): 17, np.str_('nan'): 18}\n",
      "Classes for id_24: ['11.0' '12.0' '15.0' '16.0' '17.0' '18.0' '19.0' '21.0' '23.0' '24.0'\n",
      " '25.0' '26.0' 'nan']\n",
      "Mapping for id_24: {np.str_('11.0'): 0, np.str_('12.0'): 1, np.str_('15.0'): 2, np.str_('16.0'): 3, np.str_('17.0'): 4, np.str_('18.0'): 5, np.str_('19.0'): 6, np.str_('21.0'): 7, np.str_('23.0'): 8, np.str_('24.0'): 9, np.str_('25.0'): 10, np.str_('26.0'): 11, np.str_('nan'): 12}\n",
      "Classes for id_32: ['0.0' '16.0' '24.0' '32.0' 'nan' '48.0' '8.0']\n",
      "Mapping for id_32: {np.str_('0.0'): 0, np.str_('16.0'): 1, np.str_('24.0'): 2, np.str_('32.0'): 3, np.str_('nan'): 4, np.str_('48.0'): 5, np.str_('8.0'): 6}\n",
      "Unseen categories in card1: {np.int64(2560), np.int64(9728), np.int64(1281), np.int64(11012), np.int64(5381), np.int64(3082), np.int64(14349), np.int64(11281), np.int64(2836), np.int64(8215), np.int64(6942), np.int64(9764), np.int64(2597), np.int64(8229), np.int64(9769), np.int64(13609), np.int64(14893), np.int64(1326), np.int64(13359), np.int64(12851), np.int64(11063), np.int64(6459), np.int64(16704), np.int64(8772), np.int64(3397), np.int64(15686), np.int64(9287), np.int64(15182), np.int64(8783), np.int64(17745), np.int64(15954), np.int64(14931), np.int64(4691), np.int64(2132), np.int64(9301), np.int64(17491), np.int64(6232), np.int64(9044), np.int64(16987), np.int64(5733), np.int64(6247), np.int64(17512), np.int64(13417), np.int64(16231), np.int64(10350), np.int64(14705), np.int64(3962), np.int64(4475), np.int64(6015), np.int64(16770), np.int64(4744), np.int64(12937), np.int64(7311), np.int64(13458), np.int64(13459), np.int64(16025), np.int64(10138), np.int64(8347), np.int64(16797), np.int64(8607), np.int64(17568), np.int64(3744), np.int64(6061), np.int64(16558), np.int64(6576), np.int64(16305), np.int64(14259), np.int64(5812), np.int64(8886), np.int64(12470), np.int64(2746), np.int64(7361), np.int64(5571), np.int64(14534), np.int64(9161), np.int64(13265), np.int64(4307), np.int64(7635), np.int64(5333), np.int64(13013), np.int64(16851), np.int64(18136), np.int64(11481), np.int64(5854), np.int64(4319), np.int64(4320), np.int64(1507), np.int64(5862), np.int64(7402), np.int64(11242), np.int64(16620), np.int64(5099), np.int64(13039), np.int64(2801), np.int64(1012), np.int64(8182), np.int64(16632), np.int64(5113), np.int64(9726)}\n",
      "Unseen categories in card2: {np.float64(257.0), np.float64(514.0), np.float64(516.0), np.float64(517.0), np.float64(526.0), np.float64(270.0), np.float64(530.0), np.float64(531.0), np.float64(535.0), np.float64(537.0), np.float64(282.0), np.float64(545.0), np.float64(546.0), np.float64(291.0), np.float64(549.0), np.float64(295.0), np.float64(552.0), np.float64(553.0), np.float64(554.0), np.float64(298.0), np.float64(556.0), np.float64(304.0), np.float64(307.0), np.float64(567.0), np.float64(311.0), np.float64(315.0), np.float64(316.0), np.float64(318.0), np.float64(575.0), np.float64(nan), np.float64(330.0), np.float64(588.0), np.float64(591.0), np.float64(594.0), np.float64(595.0), np.float64(597.0), np.float64(341.0), np.float64(343.0), np.float64(345.0), np.float64(347.0), np.float64(351.0), np.float64(101.0), np.float64(357.0), np.float64(361.0), np.float64(106.0), np.float64(107.0), np.float64(368.0), np.float64(117.0), np.float64(373.0), np.float64(123.0), np.float64(127.0), np.float64(386.0), np.float64(132.0), np.float64(134.0), np.float64(135.0), np.float64(136.0), np.float64(398.0), np.float64(145.0), np.float64(402.0), np.float64(149.0), np.float64(405.0), np.float64(409.0), np.float64(157.0), np.float64(414.0), np.float64(415.0), np.float64(416.0), np.float64(421.0), np.float64(171.0), np.float64(430.0), np.float64(177.0), np.float64(184.0), np.float64(187.0), np.float64(454.0), np.float64(456.0), np.float64(458.0), np.float64(206.0), np.float64(207.0), np.float64(462.0), np.float64(210.0), np.float64(212.0), np.float64(219.0), np.float64(477.0), np.float64(479.0), np.float64(224.0), np.float64(225.0), np.float64(482.0), np.float64(227.0), np.float64(228.0), np.float64(231.0), np.float64(487.0), np.float64(488.0), np.float64(490.0), np.float64(241.0), np.float64(499.0), np.float64(247.0)}\n",
      "Unseen categories in card3: {np.float64(101.0), np.float64(102.0), np.float64(103.0), np.float64(104.0), np.float64(105.0), np.float64(106.0), np.float64(107.0), np.float64(108.0), np.float64(110.0), np.float64(114.0), np.float64(115.0), np.float64(116.0), np.float64(118.0), np.float64(119.0), np.float64(122.0), np.float64(124.0), np.float64(126.0), np.float64(127.0), np.float64(128.0), np.float64(130.0), np.float64(131.0), np.float64(133.0), np.float64(135.0), np.float64(136.0), np.float64(137.0), np.float64(140.0), np.float64(144.0), np.float64(145.0), np.float64(150.0), np.float64(151.0), np.float64(162.0), np.float64(164.0), np.float64(166.0), np.float64(173.0), np.float64(174.0), np.float64(175.0), np.float64(176.0), np.float64(177.0), np.float64(180.0), np.float64(181.0), np.float64(183.0), np.float64(185.0), np.float64(187.0), np.float64(188.0), np.float64(189.0), np.float64(191.0), np.float64(194.0), np.float64(198.0), np.float64(199.0), np.float64(200.0), np.float64(201.0), np.float64(203.0), np.float64(204.0), np.float64(207.0), np.float64(208.0), np.float64(210.0), np.float64(213.0), np.float64(214.0), np.float64(216.0), np.float64(nan), np.float64(218.0), np.float64(217.0), np.float64(223.0), np.float64(229.0), np.float64(231.0)}\n",
      "Unseen categories in card5: {np.float64(101.0), np.float64(104.0), np.float64(106.0), np.float64(107.0), np.float64(110.0), np.float64(114.0), np.float64(117.0), np.float64(118.0), np.float64(119.0), np.float64(122.0), np.float64(125.0), np.float64(129.0), np.float64(130.0), np.float64(131.0), np.float64(133.0), np.float64(134.0), np.float64(136.0), np.float64(137.0), np.float64(138.0), np.float64(139.0), np.float64(142.0), np.float64(143.0), np.float64(144.0), np.float64(146.0), np.float64(149.0), np.float64(150.0), np.float64(153.0), np.float64(154.0), np.float64(155.0), np.float64(156.0), np.float64(157.0), np.float64(159.0), np.float64(161.0), np.float64(164.0), np.float64(169.0), np.float64(174.0), np.float64(176.0), np.float64(177.0), np.float64(183.0), np.float64(184.0), np.float64(186.0), np.float64(188.0), np.float64(190.0), np.float64(191.0), np.float64(192.0), np.float64(193.0), np.float64(195.0), np.float64(197.0), np.float64(199.0), np.float64(201.0), np.float64(203.0), np.float64(204.0), np.float64(213.0), np.float64(214.0), np.float64(215.0), np.float64(218.0), np.float64(219.0), np.float64(222.0), np.float64(223.0), np.float64(226.0), np.float64(228.0), np.float64(230.0), np.float64(232.0), np.float64(236.0), np.float64(237.0)}\n",
      "Unseen categories in addr1: {np.float64(512.0), np.float64(256.0), np.float64(258.0), np.float64(260.0), np.float64(510.0), np.float64(264.0), np.float64(266.0), np.float64(267.0), np.float64(524.0), np.float64(270.0), np.float64(272.0), np.float64(533.0), np.float64(537.0), np.float64(539.0), np.float64(287.0), np.float64(293.0), np.float64(296.0), np.float64(302.0), np.float64(305.0), np.float64(315.0), np.float64(318.0), np.float64(324.0), np.float64(333.0), np.float64(336.0), np.float64(343.0), np.float64(344.0), np.float64(346.0), np.float64(354.0), np.float64(359.0), np.float64(361.0), np.float64(366.0), np.float64(367.0), np.float64(370.0), np.float64(115.0), np.float64(121.0), np.float64(123.0), np.float64(380.0), np.float64(387.0), np.float64(396.0), np.float64(143.0), np.float64(399.0), np.float64(146.0), np.float64(148.0), np.float64(404.0), np.float64(150.0), np.float64(405.0), np.float64(406.0), np.float64(415.0), np.float64(nan), np.float64(174.0), np.float64(175.0), np.float64(435.0), np.float64(438.0), np.float64(442.0), np.float64(443.0), np.float64(449.0), np.float64(450.0), np.float64(453.0), np.float64(199.0), np.float64(202.0), np.float64(204.0), np.float64(208.0), np.float64(465.0), np.float64(212.0), np.float64(469.0), np.float64(472.0), np.float64(473.0), np.float64(475.0), np.float64(478.0), np.float64(480.0), np.float64(481.0), np.float64(483.0), np.float64(485.0), np.float64(487.0), np.float64(492.0), np.float64(240.0), np.float64(503.0), np.float64(505.0), np.float64(252.0), np.float64(254.0)}\n",
      "Unseen categories in addr2: {np.float64(11.0), np.float64(13.0), np.float64(19.0), np.float64(24.0), np.float64(27.0), np.float64(29.0), np.float64(30.0), np.float64(31.0), np.float64(32.0), np.float64(33.0), np.float64(34.0), np.float64(37.0), np.float64(40.0), np.float64(41.0), np.float64(44.0), np.float64(47.0), np.float64(48.0), np.float64(51.0), np.float64(nan), np.float64(58.0), np.float64(61.0), np.float64(62.0), np.float64(64.0), np.float64(65.0), np.float64(66.0), np.float64(67.0), np.float64(69.0), np.float64(71.0), np.float64(72.0), np.float64(73.0), np.float64(74.0), np.float64(77.0), np.float64(78.0), np.float64(79.0), np.float64(80.0), np.float64(81.0), np.float64(84.0), np.float64(86.0), np.float64(87.0), np.float64(88.0), np.float64(90.0), np.float64(91.0), np.float64(93.0), np.float64(94.0), np.float64(95.0), np.float64(96.0), np.float64(98.0), np.float64(99.0), np.float64(100.0), np.float64(102.0)}\n",
      "Unseen categories in P_emaildomain: {'prodigy.net.mx', 'yahoo.com', 'icloud.com', 'scranton.edu', 'mac.com', 'live.com', 'outlook.com', 'hotmail.fr', 'sc.rr.com', 'q.com', 'gmail', 'cox.net', 'twc.com', 'yahoo.es', 'anonymous.com', 'protonmail.com', 'yahoo.co.jp', 'aol.com', 'windstream.net', 'earthlink.net', 'roadrunner.com', 'outlook.es', 'hotmail.com', 'embarqmail.com', 'aim.com', 'web.de', 'ymail.com', 'netzero.net', 'hotmail.es', 'cableone.net', 'hotmail.co.uk', 'rocketmail.com', 'live.fr', 'verizon.net', 'comcast.net', 'mail.com', 'gmx.de', 'netzero.com', 'me.com', 'centurylink.net', 'yahoo.fr', 'frontiernet.net', 'suddenlink.net', 'live.com.mx', nan, 'optonline.net', 'sbcglobal.net', 'att.net', 'bellsouth.net'}\n",
      "Unseen categories in R_emaildomain: {'prodigy.net.mx', 'yahoo.com', 'icloud.com', nan, 'live.com', 'hotmail.es', 'cableone.net', 'hotmail.co.uk', 'netzero.com', 'servicios-ta.com', 'sc.rr.com', 'me.com', 'centurylink.net', 'cox.net', 'frontiernet.net', 'yahoo.es', 'protonmail.com', 'live.com.mx', 'yahoo.com.mx', 'yahoo.co.jp', 'aol.com', 'att.net'}\n",
      "Unseen categories in DeviceInfo: {'A37f', 'Rex Build/MRA58K', 'SM-G930VC', 'LDN-L21', 'SM-G928V', 'Aquaris X5 Plus Build/NMF26F', 'rv:59.0', 'SM-N900V', 'LG-TP450 Build/NRD90U', 'HUAWEI TAG-L13', 'Ilium LT500 Build/LMY47O', 'SM-J500M Build/MMB29M', nan, 'SAMSUNG-SM-G930AZ', 'Energy', 'SM-G920T', 'LGLS992', 'Build/OPR1.170623.032', 'moto e5 play Build/OPGS28.54-19-2', 'Redmi Note 3 Build/MMB29M', 'SM-J700M Build/MMB29K', 'Moto G (4) Build/NPJ25.93-14.7', 'LGL62VL'}\n",
      "Unseen categories in id_13: {np.float64(14.0), np.float64(16.0), np.float64(20.0), np.float64(nan), np.float64(24.0), np.float64(27.0), np.float64(28.0), np.float64(31.0), np.float64(36.0), np.float64(37.0), np.float64(39.0), np.float64(42.0), np.float64(44.0), np.float64(48.0), np.float64(51.0), np.float64(52.0), np.float64(55.0), np.float64(56.0), np.float64(62.0), np.float64(63.0)}\n",
      "Unseen categories in id_14: {np.float64(-120.0), np.float64(330.0), np.float64(-180.0), np.float64(270.0), np.float64(660.0), np.float64(-300.0), np.float64(600.0), np.float64(-420.0), np.float64(420.0), np.float64(-540.0), np.float64(360.0), np.float64(nan), np.float64(300.0), np.float64(-210.0), np.float64(240.0), np.float64(180.0), np.float64(120.0), np.float64(570.0)}\n",
      "Unseen categories in id_17: {np.float64(128.0), np.float64(129.0), np.float64(137.0), np.float64(141.0), np.float64(212.0), np.float64(nan), np.float64(214.0), np.float64(215.0), np.float64(217.0), np.float64(218.0), np.float64(155.0), np.float64(158.0), np.float64(159.0), np.float64(167.0), np.float64(170.0), np.float64(171.0), np.float64(172.0), np.float64(107.0), np.float64(109.0), np.float64(180.0), np.float64(117.0), np.float64(182.0), np.float64(183.0), np.float64(184.0), np.float64(122.0), np.float64(187.0), np.float64(188.0), np.float64(127.0)}\n",
      "Unseen categories in id_19: {np.float64(128.0), np.float64(389.0), np.float64(133.0), np.float64(197.0), np.float64(393.0), np.float64(523.0), np.float64(395.0), np.float64(464.0), np.float64(535.0), np.float64(224.0), np.float64(482.0), np.float64(357.0), np.float64(550.0), np.float64(231.0), np.float64(558.0), np.float64(372.0), np.float64(373.0), np.float64(502.0), np.float64(379.0), np.float64(nan), np.float64(318.0)}\n",
      "Unseen categories in id_20: {np.float64(265.0), np.float64(523.0), np.float64(268.0), np.float64(653.0), np.float64(529.0), np.float64(276.0), np.float64(405.0), np.float64(151.0), np.float64(152.0), np.float64(283.0), np.float64(157.0), np.float64(159.0), np.float64(549.0), np.float64(422.0), np.float64(171.0), np.float64(303.0), np.float64(562.0), np.float64(439.0), np.float64(311.0), np.float64(184.0), np.float64(186.0), np.float64(572.0), np.float64(453.0), np.float64(584.0), np.float64(587.0), np.float64(206.0), np.float64(463.0), np.float64(208.0), np.float64(223.0), np.float64(360.0), np.float64(104.0), np.float64(107.0), np.float64(240.0), np.float64(629.0), np.float64(nan), np.float64(378.0)}\n",
      "Unseen categories in id_21: {np.float64(202.0), np.float64(nan)}\n",
      "Unseen categories in id_22: {np.float64(nan), np.float64(39.0)}\n",
      "Unseen categories in id_25: {np.float64(153.0), np.float64(283.0), np.float64(nan)}\n",
      "Unseen categories in id_26: {np.float64(112.0), np.float64(nan)}\n",
      "Unseen categories in id_30: {'iOS 11.2.0', 'Windows Vista', nan, 'iOS 12.1.0', 'Mac OS X 10_11_4', 'iOS 10.2.1', 'Android 5.0', 'Windows XP', 'Android 7.0', 'Mac OS X 10.14', 'Mac OS X 10_14_2'}\n",
      "Unseen categories in id_31: {'chrome 58.0', 'ie 11.0 for tablet', 'ie', 'google search application 52.0', 'chrome 58.0 for android', 'firefox 62.0', 'safari', 'chrome 65.0', 'uc', 'android browser 4.0', 'opera 56.0', 'chrome 68.0 for ios', 'google search application 56.0', 'firefox 52.0', 'chrome 68.0', 'chrome', 'chrome 71.0 for ios', 'chrome 67.0 for android', nan, 'chrome 68.0 for android', 'chrome 46.0 for android', 'chrome 59.0', 'chrome 63.0 for android', 'google search application 58.0', 'chrome 55.0', 'mobile safari 9.0', 'chrome 70.0 for ios', 'rim', 'chrome 69.0', 'chrome 67.0'}\n",
      "Unseen categories in id_33: {'2160x1440', '1408x1126', '3000x1687', nan, '1800x1440', '2047x1152', '2592x1728', '1024x767', '2560x1418', '1601x900', '1368x912', '1280x768', '1920x1081', '2816x1584', '1680x945'}\n",
      "Target encoding applied to test data for high-cardinality features: ['card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'DeviceInfo', 'id_13', 'id_14', 'id_17', 'id_19', 'id_20', 'id_21', 'id_22', 'id_25', 'id_26', 'id_30', 'id_31', 'id_33']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:215: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:218: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
      "/tmp/ipykernel_10958/3354259600.py:227: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['TransactionDT_days'] = tsIdTr['TransactionDT'] // (24 * 60 * 60)  # Convert to days\n",
      "/tmp/ipykernel_10958/3354259600.py:231: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['TransactionDT_weekday'] = tsIdTr['TransactionDT_days'] % 7  # Day of the week (0-6)\n",
      "/tmp/ipykernel_10958/3354259600.py:232: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['TransactionDT_hour'] = (tsIdTr['TransactionDT']//3600) % 24  # Calculate hour of the day (0-23)\n",
      "/tmp/ipykernel_10958/3354259600.py:235: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['TransactionDT_hours'] = (tsIdTr['TransactionDT'] // (60 * 60)) % 24  # Extract hours - hours since reference point in dataset\n",
      "/tmp/ipykernel_10958/3354259600.py:237: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['hour_sin'] = np.sin(2 * np.pi * tsIdTr['TransactionDT_hour'] / 24)\n",
      "/tmp/ipykernel_10958/3354259600.py:238: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['hour_cos'] = np.cos(2 * np.pi * tsIdTr['TransactionDT_hour'] / 24)\n",
      "/tmp/ipykernel_10958/3354259600.py:239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['weekday_sin'] = np.sin(2 * np.pi * tsIdTr['TransactionDT_weekday'] / 7)\n",
      "/tmp/ipykernel_10958/3354259600.py:240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['weekday_cos'] = np.cos(2 * np.pi * tsIdTr['TransactionDT_weekday'] / 7)\n",
      "/tmp/ipykernel_10958/3354259600.py:243: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['TransactionAmt_dollars'] = tsIdTr['TransactionAmt'] // 1  # Extract whole dollars\n",
      "/tmp/ipykernel_10958/3354259600.py:244: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tsIdTr['TransactionAmt_cents'] = (tsIdTr['TransactionAmt'] * 100) % 100  # Extract cents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated categorical features: ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'DeviceInfo', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'TransactionDT_weekday', 'TransactionDT_hour', 'TransactionDT_weekday', 'TransactionDT_hour']\n",
      "Updated numerical features: ['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'TransactionDT_days', 'TransactionDT_hours', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos', 'TransactionAmt_dollars', 'TransactionAmt_cents', 'DeviceInfo_target_enc', 'id_30_target_enc', 'id_25_target_enc', 'id_31_target_enc', 'id_20_target_enc', 'addr2_target_enc', 'card5_target_enc', 'card2_target_enc', 'addr1_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'id_21_target_enc', 'card3_target_enc', 'id_17_target_enc', 'P_emaildomain_target_enc', 'id_33_target_enc', 'id_22_target_enc', 'id_26_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc', 'DeviceInfo_target_enc', 'id_30_target_enc', 'id_25_target_enc', 'id_31_target_enc', 'id_20_target_enc', 'addr2_target_enc', 'card5_target_enc', 'card2_target_enc', 'addr1_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'id_21_target_enc', 'card3_target_enc', 'id_17_target_enc', 'P_emaildomain_target_enc', 'id_33_target_enc', 'id_22_target_enc', 'id_26_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc', 'id_14_target_enc']\n",
      "Features not included in either categorical or numerical features: set()\n",
      "Removed columns: ['card1', 'card2', 'card3', 'card5', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'DeviceInfo', 'id_13', 'id_14', 'id_17', 'id_19', 'id_20', 'id_21', 'id_22', 'id_25', 'id_26', 'id_30', 'id_31', 'id_33', 'TransactionDT_days', 'TransactionDT_hours']\n",
      "Updated dataframe shape: (100, 461)\n",
      "(100, 461)\n",
      "Duplicated columns: ['DeviceInfo_target_enc', 'id_30_target_enc', 'id_25_target_enc', 'id_31_target_enc', 'id_20_target_enc', 'addr2_target_enc', 'card5_target_enc', 'card2_target_enc', 'addr1_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'id_21_target_enc', 'card3_target_enc', 'id_17_target_enc', 'P_emaildomain_target_enc', 'id_33_target_enc', 'id_22_target_enc', 'id_26_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc', 'TransactionDT_weekday', 'TransactionDT_hour']\n",
      "Updated dataframe shape: (100, 439)\n",
      "### Training Data Shapes ###\n",
      "Training data before preprocessing: (590540, 440)\n",
      "Training data after preprocessing: (590540, 165)\n",
      "\n",
      "### Test Data Shapes ###\n",
      "Test data before preprocessing: (100, 439)\n",
      "Test data after preprocessing: (100, 439)\n",
      "\n",
      "### Training Data Columns ###\n",
      "Columns in training data before preprocessing: 440\n",
      "Columns in training data after preprocessing: 165\n",
      "\n",
      "### Test Data Columns ###\n",
      "Columns in test data before preprocessing: 439\n",
      "Columns in test data after preprocessing: 439\n",
      "\n",
      "### Training Data Column Differences ###\n",
      "Columns removed from training data: 275\n",
      "Columns added to training data: 0\n",
      "\n",
      "### Test Data Column Differences ###\n",
      "Columns removed from test data: 0\n",
      "Columns added to test data: 0\n",
      "\n",
      "Removed columns in training data: ['TransactionID', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V19', 'V20', 'V25', 'V26', 'V27', 'V28', 'V35', 'V36', 'V41', 'V53', 'V54', 'V55', 'V56', 'V61', 'V62', 'V65', 'V66', 'V67', 'V68', 'V75', 'V76', 'V82', 'V83', 'V88', 'V89', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V124', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V141', 'V142', 'V143', 'V144', 'V145', 'V150', 'V151', 'V152', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V172', 'V173', 'V174', 'V175', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V187', 'V191', 'V192', 'V193', 'V196', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V223', 'V224', 'V225', 'V226', 'V227', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V240', 'V241', 'V245', 'V248', 'V250', 'V251', 'V253', 'V254', 'V255', 'V256', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'addr1_target_enc', 'addr2_target_enc', 'P_emaildomain_target_enc', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos', 'TransactionAmt_dollars', 'TransactionAmt_cents']\n",
      "tsIdTr4 len is: (100, 165)\n",
      "Expected features in the model: ['D7', 'D8', 'V15', 'V16', 'V17', 'V18', 'V21', 'V22', 'V23', 'V24', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V37', 'V38', 'V39', 'V40', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V57', 'V58', 'V59', 'V60', 'V63', 'V64', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V77', 'V78', 'V79', 'V80', 'V81', 'V84', 'V85', 'V86', 'V87', 'V90', 'V91', 'V92', 'V93', 'V94', 'V111', 'V112', 'V113', 'V123', 'V125', 'V139', 'V140', 'V146', 'V147', 'V148', 'V149', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V170', 'V171', 'V176', 'V184', 'V185', 'V186', 'V188', 'V189', 'V190', 'V194', 'V195', 'V197', 'V198', 'V199', 'V200', 'V201', 'V221', 'V222', 'V228', 'V229', 'V230', 'V238', 'V239', 'V242', 'V243', 'V244', 'V246', 'V247', 'V249', 'V252', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V282', 'V283', 'V302', 'V303', 'V304', 'id_01', 'DeviceInfo_target_enc', 'id_31_target_enc', 'id_20_target_enc', 'addr2_target_enc', 'card5_target_enc', 'card2_target_enc', 'addr1_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'card3_target_enc', 'id_17_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc', 'ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'id_12', 'id_15', 'id_16', 'id_18', 'id_23', 'id_24', 'id_27', 'id_28', 'id_29', 'id_32', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'TransactionDT_weekday', 'TransactionDT_hour']\n",
      "len of expected columns: : 159\n",
      "Expected features in the model: ['D7', 'D8', 'V15', 'V16', 'V17', 'V18', 'V21', 'V22', 'V23', 'V24', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V37', 'V38', 'V39', 'V40', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V57', 'V58', 'V59', 'V60', 'V63', 'V64', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V77', 'V78', 'V79', 'V80', 'V81', 'V84', 'V85', 'V86', 'V87', 'V90', 'V91', 'V92', 'V93', 'V94', 'V111', 'V112', 'V113', 'V123', 'V125', 'V139', 'V140', 'V146', 'V147', 'V148', 'V149', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V170', 'V171', 'V176', 'V184', 'V185', 'V186', 'V188', 'V189', 'V190', 'V194', 'V195', 'V197', 'V198', 'V199', 'V200', 'V201', 'V221', 'V222', 'V228', 'V229', 'V230', 'V238', 'V239', 'V242', 'V243', 'V244', 'V246', 'V247', 'V249', 'V252', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V282', 'V283', 'V302', 'V303', 'V304', 'id_01', 'DeviceInfo_target_enc', 'id_30_target_enc', 'id_25_target_enc', 'id_31_target_enc', 'id_20_target_enc', 'card5_target_enc', 'card2_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'id_21_target_enc', 'card3_target_enc', 'id_17_target_enc', 'id_33_target_enc', 'id_22_target_enc', 'id_26_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc', 'id_14_target_enc', 'ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'id_12', 'id_15', 'id_16', 'id_18', 'id_23', 'id_24', 'id_27', 'id_28', 'id_29', 'id_32', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'TransactionDT_weekday', 'TransactionDT_hour']\n",
      "len of expected columns: : 164\n",
      "Removed columns: ['id_30_target_enc', 'id_25_target_enc', 'id_21_target_enc', 'id_33_target_enc', 'id_22_target_enc', 'id_26_target_enc', 'id_14_target_enc']\n",
      "Expected features in the model: ['D7', 'D8', 'V15', 'V16', 'V17', 'V18', 'V21', 'V22', 'V23', 'V24', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V37', 'V38', 'V39', 'V40', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V57', 'V58', 'V59', 'V60', 'V63', 'V64', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V77', 'V78', 'V79', 'V80', 'V81', 'V84', 'V85', 'V86', 'V87', 'V90', 'V91', 'V92', 'V93', 'V94', 'V111', 'V112', 'V113', 'V123', 'V125', 'V139', 'V140', 'V146', 'V147', 'V148', 'V149', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V170', 'V171', 'V176', 'V184', 'V185', 'V186', 'V188', 'V189', 'V190', 'V194', 'V195', 'V197', 'V198', 'V199', 'V200', 'V201', 'V221', 'V222', 'V228', 'V229', 'V230', 'V238', 'V239', 'V242', 'V243', 'V244', 'V246', 'V247', 'V249', 'V252', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V282', 'V283', 'V302', 'V303', 'V304', 'id_01', 'DeviceInfo_target_enc', 'id_31_target_enc', 'id_20_target_enc', 'addr2_target_enc', 'card5_target_enc', 'card2_target_enc', 'addr1_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'card3_target_enc', 'id_17_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc', 'ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'id_12', 'id_15', 'id_16', 'id_18', 'id_23', 'id_24', 'id_27', 'id_28', 'id_29', 'id_32', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'TransactionDT_weekday', 'TransactionDT_hour']\n",
      "len of expected columns: : 159\n",
      "Expected features in the model: ['D7', 'D8', 'V15', 'V16', 'V17', 'V18', 'V21', 'V22', 'V23', 'V24', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V37', 'V38', 'V39', 'V40', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V57', 'V58', 'V59', 'V60', 'V63', 'V64', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V77', 'V78', 'V79', 'V80', 'V81', 'V84', 'V85', 'V86', 'V87', 'V90', 'V91', 'V92', 'V93', 'V94', 'V111', 'V112', 'V113', 'V123', 'V125', 'V139', 'V140', 'V146', 'V147', 'V148', 'V149', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V170', 'V171', 'V176', 'V184', 'V185', 'V186', 'V188', 'V189', 'V190', 'V194', 'V195', 'V197', 'V198', 'V199', 'V200', 'V201', 'V221', 'V222', 'V228', 'V229', 'V230', 'V238', 'V239', 'V242', 'V243', 'V244', 'V246', 'V247', 'V249', 'V252', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V282', 'V283', 'V302', 'V303', 'V304', 'id_01', 'DeviceInfo_target_enc', 'id_31_target_enc', 'id_20_target_enc', 'card5_target_enc', 'card2_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'card3_target_enc', 'id_17_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc', 'ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'id_12', 'id_15', 'id_16', 'id_18', 'id_23', 'id_24', 'id_27', 'id_28', 'id_29', 'id_32', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'TransactionDT_weekday', 'TransactionDT_hour']\n",
      "len of expected columns: : 157\n",
      "Removed columns: ['addr2_target_enc', 'addr1_target_enc']\n"
     ]
    }
   ],
   "source": [
    "# def preprocess(df):\n",
    "\"\"\"Preprocessing logic for the chunk of data.\"\"\"\n",
    "# Example preprocessing logic\n",
    "# tsIdTr = df.copy()  ###################################################################\n",
    "print(tsIdTr)\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Load the preprocess_info dictionary\n",
    "with open('/home/watoomi/SafeBank/2-data-stream/c-preprocess-classify/preprocess_info/preprocess_info.pkl', 'rb') as file:\n",
    "    preprocess_info = pkl.load(file)\n",
    "\n",
    "## ONCE OFF: rename the columns \n",
    "\n",
    "# Identify columns in tsIdTr that need renaming (those with hyphens)\n",
    "mismatched_columns = {'id-22', 'id-16', 'id-17', 'id-13', 'id-20', 'id-34', 'id-03', 'id-21', 'id-01', 'id-06', 'id-27', 'id-02', 'id-26', \n",
    "                    'id-14', 'id-35', 'id-10', 'id-05', 'id-33', 'id-31', 'id-30', 'id-12', 'id-19', 'id-23', 'id-36', 'id-38', 'id-37', \n",
    "                    'id-07', 'id-24', 'id-11', 'id-15', 'id-25', 'id-32', 'id-09', 'id-18', 'id-08', 'id-29', 'id-28', 'id-04'}\n",
    "\n",
    "\n",
    "# Create a mapping of old column names to new column names\n",
    "rename_mapping = {col: col.replace('-', '_') for col in mismatched_columns}\n",
    "\n",
    "# Rename columns in tsIdTr\n",
    "tsIdTr.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# Verify the renaming\n",
    "print(\"Updated columns in tsIdTr:\", list(tsIdTr.columns))\n",
    "\n",
    "# Convert both lists to sets\n",
    "set_tsIdTr = set(tsIdTr.columns)\n",
    "set_loaded_columns = set(preprocess_info['trIdTr_initial_columns'])\n",
    "\n",
    "# Find columns in trIdTr but not in loaded_columns\n",
    "missing_in_loaded = set_tsIdTr - set_loaded_columns\n",
    "\n",
    "# Find columns in loaded_columns but not in tsIdTr\n",
    "missing_in_tsIdTr = set_loaded_columns - set_tsIdTr\n",
    "\n",
    "# Print results\n",
    "print(\"Columns in tsIdTr but not in loaded_columns:\", missing_in_loaded)\n",
    "print(\"Columns in loaded_columns but not in tsIdTr:\", missing_in_tsIdTr)\n",
    "print('All sets exclude isFraud, for testing')\n",
    "\n",
    "############ NEEDED FOR test data ##########################################\n",
    "# Load the preprocess_info dictionary (assumes it's already loaded)\n",
    "# Load all necessary variables from preprocess_info\n",
    "categorical_features_base = preprocess_info['categorical_features']\n",
    "categorical_features = categorical_features_base.copy()\n",
    "\n",
    "numerical_features_base = preprocess_info['numerical_features']\n",
    "numerical_features = numerical_features_base.copy()\n",
    "\n",
    "label_encoders = preprocess_info['label_encoders']\n",
    "high_cardinality_features = preprocess_info['high_cardinality_features']\n",
    "low_cardinality_features = preprocess_info['low_cardinality_features']\n",
    "target_encodings = preprocess_info['target_encodings']\n",
    "column_types = preprocess_info['column_types']\n",
    "\n",
    "############## EXPERIMENTATION - verify that label encoders work correctly\n",
    "\n",
    "feature = 'ProductCD'\n",
    "\n",
    "# Get the label encoder for the feature\n",
    "le = label_encoders.get(feature)\n",
    "\n",
    "if le:\n",
    "    print(f\"Classes for {feature}: {le.classes_}\")\n",
    "    print(f\"Mapping for {feature}: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    print(f\"Label encoder for {feature} not found.\")\n",
    "\n",
    "\n",
    "# prepare and save data types of each column: \n",
    "# Assuming column_types is the dictionary from the preprocess_info_to_update['column_types']\n",
    "for col, dtype in column_types.items():\n",
    "    if col in tsIdTr.columns:\n",
    "        tsIdTr[col] = tsIdTr[col].astype(dtype)\n",
    "\n",
    "# Check if the column types have been updated\n",
    "print(tsIdTr.dtypes)\n",
    "\n",
    "print('type of dataframe is ' + str(type(tsIdTr)))\n",
    "\n",
    "# print('saving to csv')\n",
    "# tsIdTr.to_csv('tsIdTr_ex.csv')\n",
    "\n",
    "## imprvoed time complexity compared to beforehand \n",
    "\n",
    "# Dictionary to track features with new unseen classes\n",
    "new_classes_tracker = {}\n",
    "\n",
    "# Apply label encoding to the test data\n",
    "for col in low_cardinality_features:\n",
    "    le = label_encoders.get(col)  # Load the pre-saved encoder from preprocess_info\n",
    "    if le:\n",
    "        print(f\"Applying label encoding to feature: {col}\")\n",
    "\n",
    "        # Convert column values to strings for compatibility with the label encoder\n",
    "        col_values = tsIdTr[col].astype(str)  # Ensure values are strings\n",
    "        \n",
    "        # Track unseen labels\n",
    "        unseen_labels = set(col_values) - set(le.classes_)\n",
    "        if unseen_labels:\n",
    "            print(f\"Unseen labels in {col}: {unseen_labels}\")\n",
    "            new_classes_tracker[col] = list(unseen_labels)\n",
    "        \n",
    "        # Append unseen labels to the encoder's classes\n",
    "        le_classes = np.array(list(le.classes_))  # Convert the classes to a NumPy array\n",
    "        all_classes = np.concatenate([le_classes, list(unseen_labels)])  # Concatenate as arrays\n",
    "        \n",
    "        # Rebuild the label encoder with all classes\n",
    "        le.classes_ = all_classes\n",
    "        \n",
    "        # Perform label encoding with unseen labels mapped to -1\n",
    "        tsIdTr[col] = col_values.apply(\n",
    "            lambda x: le.transform([x])[0] if x in le_classes else -1\n",
    "        )\n",
    "\n",
    "\n",
    "# # Dictionary to track features with new unseen classes\n",
    "# new_classes_tracker = {}\n",
    "\n",
    "# # Apply label encoding to the test data\n",
    "# for col in low_cardinality_features:\n",
    "#     le = label_encoders.get(col)  # Load the pre-saved encoder from preprocess_info\n",
    "#     if le:\n",
    "#         print(f\"Applying label encoding to feature: {col}\")\n",
    "\n",
    "#         # Convert column values to strings for compatibility with the label encoder\n",
    "#         col_values = tsIdTr[col].astype(str) # .tolist()  Convert to a list, avoiding NumPy array\n",
    "\n",
    "#         # Track unseen labels\n",
    "#         unseen_labels = set(col_values) - set(le.classes_)\n",
    "#         if unseen_labels:\n",
    "#             print(f\"Unseen labels in {col}: {unseen_labels}\")\n",
    "#             new_classes_tracker[col] = list(unseen_labels)\n",
    "        \n",
    "#         # Append unseen labels to the encoder's classes\n",
    "#         le_classes = list(le.classes_)  # Convert the classes to a list\n",
    "#         all_classes = le_classes + list(unseen_labels)  # Concatenate as lists\n",
    "        \n",
    "#         # Rebuild the label encoder with all classes\n",
    "#         le.classes_ = all_classes\n",
    "        \n",
    "#         # Perform label encoding with unseen labels mapped to -1\n",
    "#         tsIdTr[col] = tsIdTr[col].apply(lambda x: le.transform([x])[0] if x in le_classes else -1)\n",
    "\n",
    "# # Dictionary to track features with new unseen classes\n",
    "# new_classes_tracker = {}\n",
    "\n",
    "# # Apply label encoding to the test data\n",
    "# for col in low_cardinality_features:\n",
    "#     le = label_encoders.get(col)  # Load the pre-saved encoder from preprocess_info\n",
    "#     if le:\n",
    "#         print(f\"Applying label encoding to feature: {col}\")\n",
    "\n",
    "#         # Convert column values to strings for compatibility with the label encoder\n",
    "#         col_values = tsIdTr[col].astype(str).values\n",
    "\n",
    "#         # Track unseen labels\n",
    "#         unseen_labels = set(col_values) - set(le.classes_)\n",
    "#         if unseen_labels:\n",
    "#             print(f\"Unseen labels in {col}: {unseen_labels}\")\n",
    "#             new_classes_tracker[col] = list(unseen_labels)\n",
    "        \n",
    "#         # Append unseen labels to the encoder's classes\n",
    "#         le_classes = np.array(le.classes_)\n",
    "#         all_classes = np.concatenate([le_classes, list(unseen_labels)])\n",
    "        \n",
    "#         # Rebuild the label encoder with all classes\n",
    "#         le.classes_ = all_classes\n",
    "        \n",
    "#         # Perform label encoding with unseen labels mapped to -1\n",
    "#         tsIdTr[col] = np.where(\n",
    "#             np.isin(col_values, le_classes),\n",
    "#             le.transform(col_values),\n",
    "#             -1\n",
    "#         )\n",
    "\n",
    "############## EXPERIMENTATION - verify that label encoders work correctly - finding out unseen labels in classes\n",
    "classes_with_unseen = ['id_18','id_24','id_32']\n",
    "\n",
    "for col in classes_with_unseen:\n",
    "    feature = col\n",
    "\n",
    "    # Get the label encoder for the feature\n",
    "    le = label_encoders.get(feature)\n",
    "\n",
    "    if le:\n",
    "        print(f\"Classes for {feature}: {le.classes_}\")\n",
    "        print(f\"Mapping for {feature}: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "    else:\n",
    "        print(f\"Label encoder for {feature} not found.\")\n",
    "\n",
    "# Dictionary to track new classes for target encoding (in case there are unseen categories in the test data)\n",
    "new_target_classes_tracker = {}\n",
    "\n",
    "# Apply target encoding for high-cardinality features\n",
    "for col in high_cardinality_features:\n",
    "    if col in target_encodings:\n",
    "        encoding_map = target_encodings[col]\n",
    "        \n",
    "        # Check for unseen categories in the test set\n",
    "        unseen_categories = set(tsIdTr[col].unique()) - set(encoding_map.keys())\n",
    "        if unseen_categories:\n",
    "            print(f\"Unseen categories in {col}: {unseen_categories}\")\n",
    "            new_target_classes_tracker[col] = list(unseen_categories)\n",
    "        \n",
    "        # Compute the target encoding (mean of the target variable for each category)\n",
    "        # We assume target encoding was computed based on the training set (hence no 'isFraud' in the test set)\n",
    "        # Use the original encoding_map and apply the target encoding to the test data.\n",
    "        \n",
    "        # If you have the target encoding directly as a dictionary, apply it:\n",
    "        tsIdTr[f'{col}_target_enc'] = tsIdTr[col].map(encoding_map)\n",
    "\n",
    "        # Handle missing categories or those not present in the encoding map\n",
    "        tsIdTr[f'{col}_target_enc'].fillna(-1, inplace=True)  # Use -1 for unseen categories (you can also use NaN)\n",
    "\n",
    "print(\"Target encoding applied to test data for high-cardinality features:\", high_cardinality_features)\n",
    "\n",
    "############ NEEDED FOR test data ######################################################\n",
    "\n",
    "### add in TransactionDT enriched columns:\n",
    "\n",
    "### numerical \n",
    "tsIdTr['TransactionDT_days'] = tsIdTr['TransactionDT'] // (24 * 60 * 60)  # Convert to days \n",
    "\n",
    "### categorical\n",
    "\n",
    "tsIdTr['TransactionDT_weekday'] = tsIdTr['TransactionDT_days'] % 7  # Day of the week (0-6)\n",
    "tsIdTr['TransactionDT_hour'] = (tsIdTr['TransactionDT']//3600) % 24  # Calculate hour of the day (0-23)\n",
    "\n",
    "### numerical\n",
    "tsIdTr['TransactionDT_hours'] = (tsIdTr['TransactionDT'] // (60 * 60)) % 24  # Extract hours - hours since reference point in dataset\n",
    "\n",
    "tsIdTr['hour_sin'] = np.sin(2 * np.pi * tsIdTr['TransactionDT_hour'] / 24)\n",
    "tsIdTr['hour_cos'] = np.cos(2 * np.pi * tsIdTr['TransactionDT_hour'] / 24)\n",
    "tsIdTr['weekday_sin'] = np.sin(2 * np.pi * tsIdTr['TransactionDT_weekday'] / 7)\n",
    "tsIdTr['weekday_cos'] = np.cos(2 * np.pi * tsIdTr['TransactionDT_weekday'] / 7)\n",
    "\n",
    "####    TransactionAmt split into whole dollars and cents (so one dollars/whole number, and one cents column)\n",
    "tsIdTr['TransactionAmt_dollars'] = tsIdTr['TransactionAmt'] // 1  # Extract whole dollars\n",
    "tsIdTr['TransactionAmt_cents'] = (tsIdTr['TransactionAmt'] * 100) % 100  # Extract cents\n",
    "\n",
    "# update the feature sets: \n",
    "### categorical_features\n",
    "### numerical_features , \n",
    "\n",
    "# Update categorical_features with newly created categorical columns\n",
    "categorical_features += [\n",
    "    'TransactionDT_weekday', 'TransactionDT_hour',  # New categorical features\n",
    "]\n",
    "\n",
    "# Update numerical_features with newly created numerical columns\n",
    "numerical_features += [\n",
    "    'TransactionDT_days', 'TransactionDT_hours',  # Existing numerical features\n",
    "    'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',  # Cyclical features\n",
    "    'TransactionAmt_dollars', 'TransactionAmt_cents',  # New features from TransactionAmt\n",
    "    'DeviceInfo_target_enc', 'id_30_target_enc', 'id_25_target_enc', 'id_31_target_enc', \n",
    "    'id_20_target_enc', 'addr2_target_enc', 'card5_target_enc', 'card2_target_enc', \n",
    "    'addr1_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'id_21_target_enc', \n",
    "    'card3_target_enc', 'id_17_target_enc', 'P_emaildomain_target_enc', 'id_33_target_enc', \n",
    "    'id_22_target_enc', 'id_26_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc',\n",
    "    'DeviceInfo_target_enc', 'id_30_target_enc', 'id_25_target_enc', 'id_31_target_enc', \n",
    "    'id_20_target_enc', 'addr2_target_enc', 'card5_target_enc', 'card2_target_enc', \n",
    "    'addr1_target_enc', 'id_13_target_enc', 'id_19_target_enc', 'id_21_target_enc', \n",
    "    'card3_target_enc', 'id_17_target_enc', 'P_emaildomain_target_enc', 'id_33_target_enc', \n",
    "    'id_22_target_enc', 'id_26_target_enc', 'R_emaildomain_target_enc', 'card1_target_enc', \n",
    "    'id_14_target_enc'\n",
    "]\n",
    "\n",
    "# Check the final lists\n",
    "print(\"Updated categorical features:\", categorical_features)\n",
    "print(\"Updated numerical features:\", numerical_features)\n",
    "\n",
    "### note TransactionDT_hours,TransactionDT_days, are not important features so should not be put into either numieracl or categorerial (but still remain in dataset)\n",
    "\n",
    "### find set which are not in either\n",
    "\n",
    "# Get all columns from the dataframe\n",
    "all_columns = set(tsIdTr.columns)\n",
    "\n",
    "# Combine categorical and numerical features\n",
    "all_features = set(categorical_features + numerical_features)\n",
    "\n",
    "# Identify features that are not in either categorical or numerical lists\n",
    "missing_features = all_columns - all_features\n",
    "\n",
    "# Print the missing features\n",
    "print(\"Features not included in either categorical or numerical features:\", missing_features)\n",
    "\n",
    "# check that all columns are in categorical_features or numerical_features; if not, remove column, print removed column names. \n",
    "# exclusions are: 'Unnamed: 0', 'isFraud', 'TransactionID' #\n",
    "\n",
    "# Remove target-encoded features from categorical_features if they exist in numerical_features (target encoded have become numerical now)\n",
    "categorical_features = [col for col in categorical_features if col + '_target_enc' not in numerical_features]\n",
    "\n",
    "# Remove TransactionDT_hours and TransactionDT_days from numerical_features\n",
    "numerical_features = [col for col in numerical_features if col not in ['TransactionDT_hours', 'TransactionDT_days']]\n",
    "\n",
    "# Create a set of all columns that should be included\n",
    "all_columns = set(categorical_features + numerical_features)\n",
    "\n",
    "# Exclude specific columns from being considered\n",
    "excluded_columns = {'TransactionID'} # 'isFraud', here usually but excluded for testing\n",
    "\n",
    "# Remove any columns from the dataframe that are not in the updated feature sets\n",
    "removed_columns = []\n",
    "for col in tsIdTr.columns:\n",
    "    if col not in all_columns and col not in excluded_columns:\n",
    "        removed_columns.append(col)\n",
    "        tsIdTr.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Print the names of removed columns\n",
    "print(\"Removed columns:\", removed_columns)\n",
    "\n",
    "# Verify that all remaining columns are accounted for\n",
    "remaining_columns = set(tsIdTr.columns)\n",
    "assert all(col in all_columns or col in excluded_columns for col in remaining_columns), \"Some columns are not in either feature list!\"\n",
    "\n",
    "# Remove 'TransactionDT' and 'TransactionAmt' from numerical_features\n",
    "numerical_features = [col for col in numerical_features if col not in ['TransactionDT', 'TransactionAmt']]\n",
    "\n",
    "# Drop 'TransactionDT' and 'TransactionAmt' from the DataFrame\n",
    "tsIdTr.drop(['TransactionDT', 'TransactionAmt'], axis=1, inplace=True)\n",
    "\n",
    "# create copy\n",
    "tsIdTr2 = tsIdTr.copy()\n",
    "\n",
    "low_correlation_features = preprocess_info['low_correlation_features']\n",
    "\n",
    "########## duplicated here: \n",
    "# Remove low correlation features from the test set (tsIdTr)\n",
    "tsIdTr2 = tsIdTr2.drop(columns=low_correlation_features, errors='ignore')\n",
    "\n",
    "# Remove features with low correlation\n",
    "numerical_features = [feature for feature in numerical_features if feature not in low_correlation_features]\n",
    "\n",
    "## update tsIdTr2 to include features only needed:\n",
    "\n",
    "# Define the columns to include\n",
    "included_columns = [col for col in numerical_features + categorical_features if col != 'isFraud']\n",
    "\n",
    "# 'Unnamed: 0',\n",
    "\n",
    "# Update tsIdTr2 by selecting only the desired columns\n",
    "tsIdTr2 = tsIdTr2[included_columns]\n",
    "\n",
    "# Print the shape of the updated dataframe\n",
    "print(f\"Updated dataframe shape: {tsIdTr2.shape}\")\n",
    "\n",
    "#create copy\n",
    "tsIdTr3 = tsIdTr2.copy()\n",
    "\n",
    "print(tsIdTr3.shape)\n",
    "\n",
    "# Identify duplicated columns\n",
    "duplicate_columns = tsIdTr3.columns[tsIdTr3.columns.duplicated()]\n",
    "\n",
    "# Log the duplicated columns\n",
    "print(f\"Duplicated columns: {duplicate_columns.tolist()}\")\n",
    "\n",
    "# Drop duplicate columns\n",
    "tsIdTr3 = tsIdTr3.loc[:, ~tsIdTr3.columns.duplicated()]\n",
    "\n",
    "# Print the shape of the updated dataframe\n",
    "print(f\"Updated dataframe shape: {tsIdTr3.shape}\")\n",
    "\n",
    "######################### Save shapes before and after preprocessing ##########################\n",
    "# Training\n",
    "train_before_shape = preprocess_info['train_before_shape'] \n",
    "train_after_shape = preprocess_info['train_after_shape'] \n",
    "\n",
    "# Test\n",
    "test_before_shape = tsIdTr.shape\n",
    "test_after_shape = tsIdTr3.shape\n",
    "\n",
    "######################### Save columns before and after preprocessing ##########################\n",
    "\n",
    "# Save columns before and after preprocessing for training\n",
    "train_columns_before = preprocess_info['train_columns_before'] \n",
    "train_columns_after = preprocess_info['train_columns_after']\n",
    "\n",
    "# Save columns before and after preprocessing for test\n",
    "test_columns_before = tsIdTr.columns.tolist()\n",
    "test_columns_after = tsIdTr3.columns.tolist()\n",
    "\n",
    "######################### Check differences in columns ##########################\n",
    "# For training\n",
    "train_columns_removed = [col for col in preprocess_info['train_columns_before'] if col not in preprocess_info['train_columns_after']]\n",
    "train_columns_added = [col for col in preprocess_info['train_columns_after'] if col not in preprocess_info['train_columns_before']]\n",
    "\n",
    "# For test (excluding 'isFraud' in the test set)\n",
    "test_columns_removed = [col for col in test_columns_before if col not in test_columns_after and col != 'isFraud']\n",
    "test_columns_added = [col for col in test_columns_after if col not in test_columns_before and col != 'isFraud']\n",
    "\n",
    "# Save the differences to the dictionary\n",
    "train_columns_removed = preprocess_info['train_columns_removed'] \n",
    "train_columns_added = preprocess_info['train_columns_added']\n",
    "\n",
    "######################### Handle isFraud in the test set ##########################\n",
    "# If 'isFraud' is in the test columns after preprocessing, remove it (since it shouldn't be in the test set)\n",
    "if 'isFraud' in test_columns_after:\n",
    "    #### ASSERT IS THERE THE isFraud column  \n",
    "    test_columns_after.remove('isFraud')\n",
    "\n",
    "######################### Print shapes before and after preprocessing ##########################\n",
    "print(\"### Training Data Shapes ###\")\n",
    "print(f\"Training data before preprocessing: {train_before_shape}\")\n",
    "print(f\"Training data after preprocessing: {train_after_shape}\\n\")\n",
    "\n",
    "print(\"### Test Data Shapes ###\")\n",
    "print(f\"Test data before preprocessing: {test_before_shape}\")\n",
    "print(f\"Test data after preprocessing: {test_after_shape}\\n\")\n",
    "\n",
    "######################### Print columns before and after preprocessing ##########################\n",
    "print(\"### Training Data Columns ###\")\n",
    "print(f\"Columns in training data before preprocessing: {len(train_columns_before)}\")\n",
    "print(f\"Columns in training data after preprocessing: {len(train_columns_after)}\\n\")\n",
    "\n",
    "print(\"### Test Data Columns ###\")\n",
    "print(f\"Columns in test data before preprocessing: {len(test_columns_before)}\")\n",
    "print(f\"Columns in test data after preprocessing: {len(test_columns_after)}\\n\")\n",
    "\n",
    "######################### Print differences in columns ##########################\n",
    "# For training data\n",
    "print(\"### Training Data Column Differences ###\")\n",
    "print(f\"Columns removed from training data: {len(train_columns_removed)}\")\n",
    "print(f\"Columns added to training data: {len(train_columns_added)}\\n\")\n",
    "\n",
    "# For test data\n",
    "print(\"### Test Data Column Differences ###\")\n",
    "print(f\"Columns removed from test data: {len(test_columns_removed)}\")\n",
    "print(f\"Columns added to test data: {len(test_columns_added)}\\n\")\n",
    "\n",
    "# Display removed and added columns for both training and test data if needed\n",
    "if train_columns_removed:\n",
    "    print(f\"Removed columns in training data: {train_columns_removed}\")\n",
    "if train_columns_added:\n",
    "    print(f\"Added columns in training data: {train_columns_added}\")\n",
    "\n",
    "if test_columns_removed:\n",
    "    print(f\"Removed columns in test data: {test_columns_removed}\")\n",
    "if test_columns_added:\n",
    "    print(f\"Added columns in test data: {test_columns_added}\")\n",
    "\n",
    "\n",
    "# train_columns_removed = train_columns_removed.remove('TransactionID')\n",
    "# # train_columns_removed = train_columns_removed.remove('TransactionID')\n",
    "\n",
    "cols_remove = [col for col in train_columns_removed if col != 'TransactionID']\n",
    "\n",
    "tsIdTr4 = tsIdTr3.drop(columns=cols_remove) \n",
    "print('tsIdTr4 len is: '+ str(tsIdTr4.shape))\n",
    "\n",
    "# Prepare the test features (excluding the 'isFraud' column for prediction)\n",
    "X_test = tsIdTr4.drop(columns=['TransactionID']) \n",
    "\n",
    "# Step 2: Load the pre-trained LightGBM model\n",
    "model_path = '/home/watoomi/SafeBank/2-data-stream/c-preprocess-classify//preprocess_info/safeBank_lightGBM_model.txt'\n",
    "lgbm_model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "# Get the feature names the model was trained on\n",
    "expected_columns = lgbm_model.feature_name()\n",
    "# Print the expected feature names\n",
    "print(f\"Expected features in the model: {expected_columns}\")\n",
    "print(f'len of expected columns: : {len(expected_columns)}')\n",
    "\n",
    "# Get the feature names the model was trained on\n",
    "test_columns = list(X_test.columns)\n",
    "# Print the expected feature names\n",
    "print(f\"Expected features in the model: {test_columns}\")\n",
    "print(f'len of expected columns: : {len(test_columns)}')\n",
    "\n",
    "# Find the columns in the test set that are not expected by the model\n",
    "columns_to_remove = [col for col in test_columns if col not in expected_columns]\n",
    "\n",
    "# Print the columns removed and the remaining ones\n",
    "print(f\"Removed columns: {columns_to_remove}\")\n",
    "\n",
    "X_test_reduced = X_test.drop(columns=columns_to_remove)\n",
    "\n",
    "# Get the feature names the model was trained on\n",
    "expected_columns = lgbm_model.feature_name()\n",
    "# Print the expected feature names\n",
    "print(f\"Expected features in the model: {expected_columns}\")\n",
    "print(f'len of expected columns: : {len(expected_columns)}')\n",
    "\n",
    "# Get the feature names the model was trained on\n",
    "test_columns = list(X_test_reduced.columns)\n",
    "# Print the expected feature names\n",
    "print(f\"Expected features in the model: {test_columns}\")\n",
    "print(f'len of expected columns: : {len(test_columns)}')\n",
    "\n",
    "# Find the columns in the test set that are not expected by the model\n",
    "columns_to_remove = [col for col in expected_columns if col not in test_columns]\n",
    "\n",
    "# Print the columns removed and the remaining ones\n",
    "print(f\"Removed columns: {columns_to_remove}\")\n",
    "\n",
    "\n",
    "# Option 2: Add missing columns to the test set (fill with NaN or some default value)\n",
    "for col in columns_to_remove:\n",
    "    X_test_reduced[col] = np.nan  # or some placeholder value if needed\n",
    "\n",
    "# Step 3: Perform predictions on the test set\n",
    "predictions = lgbm_model.predict(X_test_reduced)\n",
    "# Probability of fraud\n",
    "probability_predictions = predictions  # The raw output probabilities from the model\n",
    "\n",
    "# Binary predictions with a custom threshold\n",
    "binary_predictions = (probability_predictions > 0.7).astype(int)\n",
    "\n",
    "# Get current time in the format YYYY-MM-DD_HH-MM-SS\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "timestamp_short = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Collect prediction metadata (add more as needed)\n",
    "prediction_metadata = {\n",
    "    'model_version': 'v_Mod'+ timestamp_short, # after hard encoded\n",
    "    'preprocessing_version': 'v_Pro'+ timestamp_short,\n",
    "    'prediction_threshold': 0.7,  # The threshold for binary classification\n",
    "    'classification_time': timestamp,\n",
    "    'model_name': 'LightGBM Fraud Detection',  # Optional model name\n",
    "    'features_used': X_test_reduced.columns.tolist(),  # Columns/features used in prediction\n",
    "}\n",
    "\n",
    "# Step 4: Prepare the DataFrame to store the predictions along with additional columns\n",
    "predictions_df = pd.DataFrame({\n",
    "    'TransactionID': tsIdTr4['TransactionID'],\n",
    "    'isFraud_Prediction': binary_predictions,\n",
    "    'Fraud_Probability': probability_predictions,  # Probability of fraud\n",
    "    'Classification_Time': timestamp,  # Time of classification\n",
    "    'Model_Version': prediction_metadata['model_version'],\n",
    "    'Prediction_Threshold': prediction_metadata['prediction_threshold'],\n",
    "    'Model_Name': prediction_metadata['model_name']\n",
    "})\n",
    "\n",
    "# return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud_Prediction</th>\n",
       "      <th>Fraud_Probability</th>\n",
       "      <th>Classification_Time</th>\n",
       "      <th>Model_Version</th>\n",
       "      <th>Prediction_Threshold</th>\n",
       "      <th>Model_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663551</td>\n",
       "      <td>0</td>\n",
       "      <td>0.399033</td>\n",
       "      <td>2024-12-20_16-46-54</td>\n",
       "      <td>v_Mod2024-12-20</td>\n",
       "      <td>0.7</td>\n",
       "      <td>LightGBM Fraud Detection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud_Prediction  Fraud_Probability  Classification_Time  \\\n",
       "0        3663551                   0           0.399033  2024-12-20_16-46-54   \n",
       "\n",
       "     Model_Version  Prediction_Threshold                Model_Name  \n",
       "0  v_Mod2024-12-20                   0.7  LightGBM Fraud Detection  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SafeBank_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
